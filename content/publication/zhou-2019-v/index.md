---
# Documentation: https://wowchemy.com/docs/managing-content/

title: V-statistics and Variance Estimation
subtitle: ''
summary: ''
authors:
- Zhengze Zhou
- Lucas Mentch
- Giles Hooker
tags: []
categories: []
date: '2019-01-01'
lastmod: 2021-06-07T23:34:17-04:00
featured: false
draft: false

url_pdf: 'https://arxiv.org/abs/1912.01089'

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-06-08T03:34:16.896127Z'
publication_types:
- '3'
abstract: 'This paper develops a general framework for analyzing asymptotics of V-statistics. Previous literature on limiting distribution mainly focuses on
the cases when n → ∞ with fixed kernel size k. Under some regularity conditions, we demonstrate asymptotic normality when k grows with n by utilizing existing results for U-statistics. The key in our approach lies in a mathematical reduction to U-statistics by designing an equivalent kernel for V-statistics. We also provide a unified treatment on variance estimation for both U- and V-statistics by observing connections to existing methods and proposing an empirically more accurate estimator. Ensemble methods such as random forests, where multiple base learners are trained and aggregated for prediction purposes, serve as a running example throughout the paper because they are a natural and flexible application of V-statistics.'
publication: '*arXiv preprint arXiv:1912.01089*'
---
